{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will start to rebuild the DDPM paper from scratch, the whole procedure of DDPM procedure can be thought as a process of demolishing a big building into bricks and rebuild the building by using those bricks, we first add noise into the model, then the model will learn how to rebuild the image back into the original one, the DDPM is actually not a typical \"diffusion model\" but a VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from functools import partial # fancy way to wrap functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DDPM paper used a very similar architecture used in UNet paper, but with several modification,  the authors replaced the original double convolutions in each encoding steps with the 'Residual Blocks' used in the Resnet paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five components in DDPM model, they are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Encoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bottlneck blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Self Attention modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sinusodial time embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the details of the model architecture: \n",
    "1. Same as original UNet model,  there are four levels in the encoder and decoder parts of DDPM model, as well as bottleneck part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the SinusodialPositionEncoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusodialPositionEncoding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 theta = 10000\n",
    "        ):\n",
    "        \"\"\"\n",
    "        dim: dimension of the input\n",
    "        theta: mentioned in the paper 'attention is all you need'   \n",
    "        \"\"\"\n",
    "        super(SinusodialPositionEncoding, self).__init__()\n",
    "        assert dim % 2 == 0, 'Dimension of input recommended to be an even number'\n",
    "        self.dim = dim\n",
    "    \n",
    "    def positional_encoding(self, x, position):\n",
    "        half_dim = self.dim//2\n",
    "        median = math.log(self.theta) / half_dim\n",
    "        position = x[:, None] # The x shape before adding None is (batch_size, seq_len)\n",
    "        emb = torch.exp(torch.arange(0, half_dim,device=x.device)* -median)\n",
    "\n",
    "        positional_encoding = torch.zeros(len(x), self.dim, device=x.device)\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * emb)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * emb)\n",
    "\n",
    "        return positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Resnet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 time_channels,\n",
    "                 kernel_size=3, \n",
    "                 padding_size=1, \n",
    "                 n_group=32, \n",
    "                 dropout=True, \n",
    "                 time_embedding=True\n",
    "        ):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.in_dim = in_channels\n",
    "        self.out_dim = out_channels\n",
    "        self.time_channels = time_channels\n",
    "        self.dropout = dropout\n",
    "        self.time_embedding = time_embedding\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding_size)\n",
    "        self.acvtivation1 = nn.SiLU()\n",
    "        self.norm1 = nn.GroupNorm(num_groups=n_group, num_channels=out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding_size)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.norm2 = nn.GroupNorm(num_groups=n_group, num_channels=out_channels)\n",
    "        self.res_connection = nn.Conv2d(in_channels, out_channels,1,0) if in_channels != out_channels else nn.Identity()\n",
    "        self.time_emb = nn.Conv2d(time_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.acvtivation1(x)\n",
    "        x = self.norm1(x)\n",
    "        if self.time_embedding:\n",
    "            time_emb = self.time_emb()\n",
    "            time_emb = rearrange(time_emb, 'b c->b c 1 1')\n",
    "            x = x + time_emb\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        if self.dropout:\n",
    "            dropout = nn.Dropout(0.1)\n",
    "            x = dropout(x)\n",
    "        x = x + self.res_connection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Attention Block with Flatten Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocusedLinearAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 num_heads=8, \n",
    "                 bias=False, \n",
    "                 scale=0, \n",
    "                 attn_drop=0.2, \n",
    "                 proj_drop=0.2,\n",
    "                 focusing_factor=3, \n",
    "                 kernel_size=5\n",
    "        ):\n",
    "        super(FocusedLinearAttention, self).__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = scale\n",
    "        self.kernel = nn.ReLU()\n",
    "        head_dim = dim // num_heads\n",
    "        \n",
    "        # requires bias = False when using linear projection\n",
    "        self.q = nn.Linear(dim, dim, bias=bias)\n",
    "        self.k = nn.Linear(dim, dim, bias=bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # Focusing on higher cosine similarity pairs\n",
    "        self.focusing_factor = focusing_factor\n",
    "        # Depthwise Convolution, padding = kernel_size//2 to make sure the image with the same size after convolution\n",
    "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
    "                             groups=head_dim, padding=kernel_size // 2)\n",
    "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, 1, dim)))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        focusing_factor = self.focusing_factor\n",
    "        scale = nn.Softplus()(self.scale)\n",
    "        q = self.kernel(q) + 1e-6\n",
    "        k = self.kernel(k) + 1e-6\n",
    "        q = q / scale\n",
    "        k = k / scale\n",
    "        q_norm = q.norm(dim=-1, keepdim=True)\n",
    "        k_norm = k.norm(dim=-1, keepdim=True)\n",
    "        q = q ** focusing_factor\n",
    "        k = k ** focusing_factor\n",
    "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
    "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
    "\n",
    "        # Rearrange into multi-head dimension, each head will have C/H dimensions\n",
    "        q, k, v = (rearrange(x, \"b n (h c) -> (b h) n c\", h=self.num_heads) for x in [q, k, v])\n",
    "        i, j, c, d = q.shape[-2], k.shape[-2], k.shape[-1], v.shape[-1]\n",
    "        # print(i, j, c, d)\n",
    "        # print(q.shape)\n",
    "        # print(k.shape)\n",
    "        # print(v.shape)\n",
    "        # print(i * j * (c + d))\n",
    "        # print(c * d * (i + j))\n",
    "        z = 1 / (torch.einsum(\"b i c, b c -> b i\", q, k.sum(dim=1)) + 1e-6)\n",
    "        # Using Linear Attention Mechanism here to get O(N) complexity\n",
    "        if i * j * (c + d) > c * d * (i + j):\n",
    "            kv = torch.einsum(\"b j c, b j d -> b c d\", k, v)\n",
    "            x = torch.einsum(\"b i c, b c d, b i -> b i d\", q, kv, z)\n",
    "        else:\n",
    "            qk = torch.einsum(\"b i c, b j c -> b i j\", q, k)\n",
    "            x = torch.einsum(\"b i j, b j d, b i -> b i d\", qk, v, z)\n",
    "\n",
    "        num = int(v.shape[1] ** 0.5)\n",
    "        feature_map = rearrange(v, \"b (w h) c -> b c w h\", w=num, h=num)\n",
    "        # Expanding the attention matrix rank from d to N to expand the expressing power of the model\n",
    "        feature_map = rearrange(self.dwc(feature_map), \"b c w h -> b (w h) c\")\n",
    "        # Adding the expanding version feature map back into the input\n",
    "        x = x + feature_map\n",
    "        x = rearrange(x, \"(b h) n c -> b n (h c)\", h=self.num_heads)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Attention Block with Self-attetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Origin_Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 input_channels,\n",
    "                 heads=8,\n",
    "                 scale=True,\n",
    "                 dropout=0.1,\n",
    "                 n_group=32,\n",
    "                 bias=False\n",
    "        ):\n",
    "        super(Origin_Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.input_channels = input_channels\n",
    "        self.head_dim = dim // heads\n",
    "        self.scale = scale  \n",
    "\n",
    "        self.qkv = nn.Linear(input_channels, dim*3, bias=bias)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(dim, input_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b c x y -> b (x y) c', c=self.input_channels) # reshape from (batch_size, channel, height, width) to (batch_size, height*width, channel)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b p (h d) -> b p h d ', h=self.heads), (q , k, v))\n",
    "        attn = torch.einsum('b i h d, b j h d -> b j i h', q, k)\n",
    "        if self.scale:\n",
    "            attn = attn / (self.head_dim) ** 0.5\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        x = torch.einsum('b i j h, b j h d -> b i h d', attn, v)\n",
    "        x = rearrange(x, 'b (x y) h d -> b h x y', x=h, y=w)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the downsample Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoders(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(Encoders, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, 3, stride=2, padding=1)\n",
    "        \n",
    "    @staticmethod\n",
    "    def initialize(x):\n",
    "        init.xavier_uniform_(x.weight)\n",
    "        init.zeros_(x.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the upsample Decoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoders(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Decoders, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, 3, stride=1, padding=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize(x):\n",
    "        init.xavier_uniform_(x.weight)\n",
    "        init.zeros_(x.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the UNet used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 init_dim,\n",
    "                 model_dim,\n",
    "                 theta=10000,\n",
    "                 attn_heads=8,\n",
    "                 resnet_blocks=8,\n",
    "                 dim_mults=(1, 2, 4, 8)\n",
    "        ):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_dim = init_dim\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.positional_encoding = SinusodialPositionEncoding(dim=model_dim, theta=theta)\n",
    "        self.attention = FocusedLinearAttention(dim=model_dim, num_heads=attn_heads)\n",
    "        self.encoders = nn.ModuleList([])\n",
    "        self.decoders = nn.ModuleList([])\n",
    "        \n",
    "        self.init_conv = nn.Conv2d(in_channels, init_dim, 7, padding=3)\n",
    "        dim = [init_dim, *map(lambda m:init_dim*m, dim_mults)]\n",
    "        dim_list = zip(dim[:-1], dim[1:])\n",
    "        \n",
    "        whole = len(dim_list)\n",
    "        last = int >= (whole - 1)\n",
    "\n",
    "        res_block = partial(ResnetBlock, n_group = resnet_blocks)\n",
    "        time_dim = dim*4 # time embedding metioned in the DDPM paper\n",
    "        \n",
    "        self.time_embedding = nn.Sequential(\n",
    "            self.positional_encoding(),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        \n",
    "        # define the encoder part of the model  \n",
    "        for i, (dim_in, dim_out) in enumerate(dim_list):\n",
    "            self.encoders.append(nn.ModuleList([\n",
    "                res_block(dim_in, dim_in, time_dim),\n",
    "                res_block(dim_in, dim_in, time_dim),\n",
    "                Decoders(dim_in) if not last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "            ]))\n",
    "\n",
    "        # define the bottleneck part of the model\n",
    "        bottleneck_dim = dim[-1]\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            res_block(bottleneck_dim, bottleneck_dim, time_dim),\n",
    "            res_block(bottleneck_dim, bottleneck_dim, time_dim) \n",
    "        )\n",
    "\n",
    "        # define the decoder part of the model\n",
    "        for i, (dim_in, dim_out) in reversed(list(enumerate(dim_list))):\n",
    "            self.decoders.append(nn.ModuleList([\n",
    "                res_block(dim_in + dim_out, dim_out, time_dim),\n",
    "                res_block(dim_in + dim_out, dim_out, time_dim),\n",
    "                Decoders(dim_in) if not last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "            ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
