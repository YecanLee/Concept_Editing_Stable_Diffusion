{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ra78lof\\AppData\\Local\\anaconda3\\envs\\Transunet\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "from pathlib import Path\n",
    "\n",
    "from diffusers.schedulers import LMSDiscreteScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
    "\n",
    "import glob\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'terrorist'\n",
    "target_seed = 55\n",
    "folder = f'./{concept}'\n",
    "prompt = f'a photo of a '\n",
    "num_inference_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipe.to(\"cuda\")\n",
    "pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "pipe.tokenizer.add_tokens('<>')\n",
    "trained_id = pipe.tokenizer.convert_tokens_to_ids('<>')\n",
    "pipe.text_encoder.resize_token_embeddings(len(pipe.tokenizer))\n",
    "_ = pipe.text_encoder.get_input_embeddings().weight.requires_grad_(False)\n",
    "\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to('cuda')\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "transform_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_transform(image_tensor):\n",
    "    image_tensor = torch.nn.functional.interpolate(image_tensor, size=(224, 224), mode='bicubic',\n",
    "                                                   align_corners=False)\n",
    "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                      std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    image_tensor = normalize(image_tensor)\n",
    "    return image_tensor\n",
    "\n",
    "def load_alphas(alphas_projection, token_embeddings, seed, prompt, avg_norm=0.1):\n",
    "    alphas_copy = alphas_projection.clone()\n",
    "    # embeddings_mat = token_embeddings[dictionary]\n",
    "    embedding = torch.matmul(alphas_copy, token_embeddings)\n",
    "    embedding = torch.mul(embedding, 1 / embedding.norm())\n",
    "    embedding = torch.mul(embedding, avg_norm)\n",
    "    pipe.text_encoder.text_model.embeddings.token_embedding.weight[trained_id] = torch.nn.Parameter(\n",
    "        embedding)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    return pipe(prompt, guidance_scale=7.5,\n",
    "                generator=generator,\n",
    "                return_dict=False,\n",
    "                num_images_per_prompt=1,\n",
    "                num_inference_steps=num_inference_steps)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.Generator:\n",
    "Purpose: A Generator is an object representing an independent random number generator. It allows for more fine-grained control of random number generation.\n",
    "Use Cases: You might use a Generator when you need to maintain multiple independent random number generators. \n",
    "For example, in a scenario where different parts of your code need to be randomized independently of each other, \n",
    "or when you want to ensure reproducibility of specific operations without affecting the global RNG state.\n",
    "Flexibility: With Generator, you can create different streams of random numbers that are not influenced by the global seed or by each other. \n",
    "This is particularly useful in parallel processing or when you want to isolate the randomness in different parts of your code.\n",
    "Usage: To use a Generator, you first create an instance of it (e.g., gen = torch.Generator()), \n",
    "optionally set its seed (e.g., gen.manual_seed(1234)), and then pass it as an argument to functions that accept a generator.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_nu = concept.replace('_', ' ')\n",
    "concept_u = concept.replace(' ', '_')\n",
    "\n",
    "orig_embeddings = pipe.text_encoder.text_model.embeddings.token_embedding.weight.clone().detach()\n",
    "norms = [i.norm().item() for i in orig_embeddings]\n",
    "avg_norm = np.mean(norms)\n",
    "\n",
    "alphas_dict = torch.load(f'{folder}/output/best_alphas.pt').detach_().requires_grad_(False)\n",
    "\n",
    "dictionary = torch.load(f'{folder}/output/dictionary.pt')\n",
    "sorted_alphas, sorted_indices = torch.sort(alphas_dict, descending=True)\n",
    "alpha_ids = []\n",
    "num_alphas = 50\n",
    "for i, idx in enumerate(sorted_indices[:num_alphas]):\n",
    "    alpha_ids.append((i, pipe.tokenizer.decode([dictionary[idx]])))\n",
    "alphas = torch.zeros(orig_embeddings.shape[0]).cuda()\n",
    "top_word_idx = [dictionary[i] for i in sorted_indices[:num_alphas]]\n",
    "for i, index in enumerate(top_word_idx):\n",
    "    alphas[index] = alphas_dict[sorted_indices[i]]\n",
    "\n",
    "clip_concept_inputs = clip_tokenizer([concept_nu], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "clip_concept_features = clip_model.get_text_features(**clip_concept_inputs)\n",
    "\n",
    "clip_text_inputs = clip_tokenizer([pipe.tokenizer.decode([x]) for x in top_word_idx], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "clip_text_features = clip_model.get_text_features(**clip_text_inputs)\n",
    "clip_words_similarity = (torch.matmul(clip_text_features, clip_text_features.transpose(1, 0)) /\n",
    "                         torch.matmul(clip_text_features.norm(dim=1).unsqueeze(1),\n",
    "                                      clip_text_features.norm(dim=1).unsqueeze(0)))\n",
    "\n",
    "concept_words_similarity = torch.cosine_similarity(clip_concept_features, clip_text_features, axis=1)\n",
    "similar_words = (np.array(concept_words_similarity.detach().cpu()) > 0.92).nonzero()[0]\n",
    "clip_words_similarity = (np.array(clip_words_similarity.detach().cpu()) > 0.95)\n",
    "\n",
    "# Zero-out similar words\n",
    "for i in similar_words:\n",
    "    alphas[top_word_idx[i]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_nu = concept.replace('_', ' ')\n",
    "concept_u = concept.replace(' ', '_')\n",
    "\n",
    "orig_embeddings = pipe.text_encoder.text_model.embeddings.token_embedding.weight.clone().detach()\n",
    "norms = [i.norm().item() for i in orig_embeddings]\n",
    "avg_norm = np.mean(norms)\n",
    "\n",
    "alphas_dict = torch.load(f'{folder}/output/best_alphas.pt').detach_().requires_grad_(False)\n",
    "\n",
    "dictionary = torch.load(f'{folder}/output/dictionary.pt')\n",
    "sorted_alphas, sorted_indices = torch.sort(alphas_dict, descending=True)\n",
    "alpha_ids = []\n",
    "num_alphas = 50\n",
    "for i, idx in enumerate(sorted_indices[:num_alphas]):\n",
    "    alpha_ids.append((i, pipe.tokenizer.decode([dictionary[idx]])))\n",
    "alphas = torch.zeros(orig_embeddings.shape[0]).cuda()\n",
    "top_word_idx = [dictionary[i] for i in sorted_indices[:num_alphas]]\n",
    "for i, index in enumerate(top_word_idx):\n",
    "    alphas[index] = alphas_dict[sorted_indices[i]]\n",
    "\n",
    "clip_concept_inputs = clip_tokenizer([concept_nu], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "clip_concept_features = clip_model.get_text_features(**clip_concept_inputs)\n",
    "\n",
    "clip_text_inputs = clip_tokenizer([pipe.tokenizer.decode([x]) for x in top_word_idx], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "clip_text_features = clip_model.get_text_features(**clip_text_inputs)\n",
    "clip_words_similarity = (torch.matmul(clip_text_features, clip_text_features.transpose(1, 0)) /\n",
    "                         torch.matmul(clip_text_features.norm(dim=1).unsqueeze(1),\n",
    "                                      clip_text_features.norm(dim=1).unsqueeze(0)))\n",
    "\n",
    "concept_words_similarity = torch.cosine_similarity(clip_concept_features, clip_text_features, axis=1)\n",
    "similar_words = (np.array(concept_words_similarity.detach().cpu()) > 0.92).nonzero()[0]\n",
    "clip_words_similarity = (np.array(clip_words_similarity.detach().cpu()) > 0.95)\n",
    "\n",
    "# Zero-out similar words\n",
    "for i in similar_words:\n",
    "    alphas[top_word_idx[i]] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transunet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
