{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, CLIPModel\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "\n",
    "concept = 'sweetpepper'\n",
    "folder = f'./{concept}'\n",
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example for torch sort\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "sorted, indices = torch.sort(x, dim=1, descending=True)\n",
    "print(sorted)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the concept is a compound word, replace the underscore with a space\n",
    "concept_nu = concept.replace('_', ' ')\n",
    "\n",
    "# Load the model, detach to make sure no grad\n",
    "orig_embeddings = pipe.text_encoder.text_model.embeddings.token_embedding.weight.clone().detach()\n",
    "\n",
    "# Get the average norm of the embeddings\n",
    "norms = [i.norm().item() for i in orig_embeddings]\n",
    "avg_norm = np.mean(norms)\n",
    "\n",
    "# alpha is the weight of each hidden concept achived from CLIP, for example: sweetpaper = 0.8 * fingers + 0.5 * paper\n",
    "alphas_dict = torch.load(f'{folder}/output/best_alphas.pt').detach_().requires_grad_(False)\n",
    "\n",
    "# Get the dictionary of the tokenizer\n",
    "dictionary = torch.load(f'{folder}/output/dictionary.pt')\n",
    "\n",
    "# the dictionary would be for example: {'finers': 0.8, 'paper': 0.1, 'sweet': 0.05,...}\n",
    "sorted_alphas, sorted_indices = torch.sort(alphas_dict, descending=True)\n",
    "alpha_ids = []\n",
    "num_alphas = 50\n",
    "\n",
    "# Get the top 50 words with the highest weights, sorted_indices is the index of the words in the dictionary\n",
    "# for example the finger was the 8th word in the dictionary, pepper was the 10th word in the dictionary, then the sorted_indices would be [8, 10, ...]\n",
    "# then we decode the top50 hidden concepts by using this code chunk\n",
    "for i, idx in enumerate(sorted_indices[:num_alphas]):\n",
    "    alpha_ids.append((i, pipe.tokenizer.decode([dictionary[idx]])))\n",
    "alphas = torch.zeros(orig_embeddings.shape[0]).cuda()\n",
    "top_word_idx = [dictionary[i] for i in sorted_indices[:num_alphas]]\n",
    "for i, index in enumerate(top_word_idx):\n",
    "    alphas[index] = alphas_dict[sorted_indices[i]]\n",
    "# after this code chunk, we get the top 50 hidden concepts and their weights, the rest of the hidden concepts are set to 0\n",
    "# the alphas would be for example: [0.8, 0.5, 0.1, 0.05, 0.01, 0.01, 0.01, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]\n",
    "\n",
    "# the clip concept input is the concept we are using now, for exmaple sweetpepper\n",
    "clip_concept_inputs = clip_tokenizer([concept_nu], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "clip_concept_features = clip_model.get_text_features(**clip_concept_inputs)\n",
    "\n",
    "# the clip text input is the top 50 hidden concepts we get from the previous code chunk\n",
    "clip_text_inputs = clip_tokenizer([pipe.tokenizer.decode([x]) for x in top_word_idx], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "clip_text_features = clip_model.get_text_features(**clip_text_inputs)\n",
    "clip_words_similarity = (torch.matmul(clip_text_features, clip_text_features.transpose(1, 0)) /\n",
    "                         torch.matmul(clip_text_features.norm(dim=1).unsqueeze(1),\n",
    "                                      clip_text_features.norm(dim=1).unsqueeze(0)))\n",
    "# the clip_words_similarity is the similarity between the top 50 hidden concepts, for example, the similarity between fingers and paper is 0.5, this \n",
    "# is calculated for the future loop, for example if one hidden concept got removed from the list, then the very similar hidden concept would also be removed\n",
    "\n",
    "# concept similarity is the similarity between the concept and the top 50 hidden concepts, I am not so sure if this is proper since we are only considering the \n",
    "# the hidden concepts instead of considering about their weights, a.k.a. the alphas\n",
    "concept_words_similarity = torch.cosine_similarity(clip_concept_features, clip_text_features, axis=1)\n",
    "similar_words = (np.array(concept_words_similarity.detach().cpu()) > 0.92).nonzero()[0]\n",
    "clip_words_similarity = (np.array(clip_words_similarity.detach().cpu()) > 0.95)\n",
    "\n",
    "# Zero-out similar words\n",
    "for i in similar_words:\n",
    "    alphas[top_word_idx[i]] = 0\n",
    "\n",
    "\n",
    "# the problem with this code chunk would be:\n",
    "# the alphas are only used for sorting the hidden concepts, but not used for calculating the similarity between the concept and the hidden concepts\n",
    "# what if the hidden concept has a very high weight, but the similarity between the concept and the hidden concept is very low, then this hidden concept\n",
    "# would not be removed from the list, but it should be removed from the list\n",
    "# also, what if several hidden concepts have very similar and very high alphas, then if we only want to pick up two hidden concepts, then it's tricky"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
